{
 "cells": [
  {
   "cell_type": "raw",
   "id": "158cd5fa-6a01-4f09-8378-132a53a2db92",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008ef8a-e93f-432c-963a-c0f99d60f186",
   "metadata": {},
   "source": [
    "**Author:** [Scott Sievert](https://stsievert.com)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e51c797f-a456-472a-a798-067eef5b797a",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b249b-ea77-4605-a932-4bf5db552615",
   "metadata": {},
   "source": [
    "Salmon and NEXT are two different software systems to perform the same task, adaptive sampling for the \"triplet embedding\" problem. Salmon and NEXT exhibit different performance when launched on EC2 as their respective documentation recommends and human responses are simulated:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77bee618-56af-46c1-8bed-52b38810ae37",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ca71860-f962-4f93-ac72-3333fb95f991",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/direct-comparison.png}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a857c58d-65da-4a4b-aafd-abb8e885057c",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0441d-844e-4e48-8aff-12d10c94c1d3",
   "metadata": {},
   "source": [
    "Accuracy represents performance on unseen (simulated) human responses. The average number of items closer than the true nearest neighbor (NN) is perhaps a measure of embedding quality. For this target set of \"alien eggs\" characterized by one parameter, having this value be 1 represents a good embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd4aa1-c8de-41fb-9d2d-758a85fa59c3",
   "metadata": {},
   "source": [
    "# Why don't Salmon and NEXT have the same performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fff99-46b6-48f7-a418-aa0b3e929f24",
   "metadata": {},
   "source": [
    "What's responsible for this different performance? Fundamentally, each software system has two tasks:\n",
    "\n",
    "* Searching for the most informative queries to ask users.\n",
    "* Generating embeddings from the user's responses.\n",
    "\n",
    "These tasks are done online as responses are being received. These systems are intertwined: the queries and query quality depend on the embedding, and the embedding depends on the responses received from each query.\n",
    "\n",
    "So, the difference in performance must come from the question **\"how do these systems differ in NEXT and Salmon?\"** I will present evidence to support the following claims:\n",
    "\n",
    "1. Salmon and NEXT's embeddings perform equally well.\n",
    "2. NEXT's search—a greedy information gain search—performs better as searches efficacy increases.\n",
    "    * However, a complete search presents experimental design challenges.\n",
    "3. To work around those challenges, Salmon employs a more exploratory search that is a modification of NEXT's greedy search. These modifications...\n",
    "     * significantly enhance Salmon's performance over NEXT's greedy search.\n",
    "     * has the same \"query diversity\" as random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b23aeb-60a0-43a5-967c-d9ee4a689996",
   "metadata": {},
   "source": [
    "Of course, each of these claims is an empirical observation for the experiments that have been run. In these experiments, responses were submitted at an average of 1 response/sec to simulate 2 simultaneous users.[^unreal]\n",
    "\n",
    "[^unreal]:The use of a constant response rate is slightly unrealistic; users take slightly longer to answer more difficult queries. In addition, submitting responses at a constant rate avoids some important architecture differences and allows a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076abd5-e505-418c-907c-ecbd5449cba5",
   "metadata": {},
   "source": [
    "Let's present evidence for each of these claims. When providing evidence, I will follow the data scientist workflow:\n",
    "\n",
    "1. Launch the software on Amazon EC2 as the documentation recommends.\n",
    "2. Collect (simulated) human responses.\n",
    "3. Download the responses.\n",
    "4. Generate an embedding offline with my own software.\n",
    "\n",
    "This document is used to motivate changes in Salmon's search. We've also used Salmon to improve the efficacy of NEXT's search, called \"NEXT proxy\" below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c24c6199-6c21-4032-83fd-3ee49dd9d206",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81d39c8c-236b-405c-ab37-47c86f961c33",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/summary-complete.png}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "174f5db3-f279-4765-823d-36cc4ae36d7d",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6717f78-8431-424e-9ebc-6052c4315526",
   "metadata": {},
   "source": [
    "We've run NEXT proxy 9 different times to vary the search efficacy, and the best of those runs is shown with \"best of NEXT proxies.\" It appears the changes made to Salmon to improve query diversity do not to degrade performance – especially relevant because Salmon's search is easier to configure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4a4c6-11e3-4401-8dbd-c32a925da92f",
   "metadata": {},
   "source": [
    "# Embedding performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1b7f9-72b0-4f01-9fc9-c43fd7c0909c",
   "metadata": {},
   "source": [
    "Let's isolate the embedding performance by running NEXT and Salmon with similar searches. That is, let's configure Salmon to have a similar search to NEXT.[^arch] Then, the relevant question is **\"do both software systems generate embeddings of similar quality?\"**\n",
    "\n",
    "[^arch]:There are some important architecture differences; submitting responses at a constant rate circumvent these issues."
   ]
  },
  {
   "cell_type": "raw",
   "id": "019bbcc1-bdb2-4c7a-86b3-1d8ba2cb864b",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e941694d-7834-49eb-804e-10121c235f12",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/same-search-diff-embedding.png}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09ea5904-9aaf-4361-9ba8-cad560842782",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007d19e-238b-439a-a71c-5d7339ca0141",
   "metadata": {},
   "source": [
    "It appears Salmon and NEXT generate equivalent embeddings, at least at this rate (1 response/sec) for (almost) equivalent searches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9a5c-bbeb-49dc-8493-11fdf56e278c",
   "metadata": {},
   "source": [
    "# Search performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3907a0-395d-417e-abce-9b1d7257df10",
   "metadata": {},
   "source": [
    "Now, let's isolate the search performance. That is, let's use the same embedding system and vary the searches by implement different searches in Salmon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d8074-b35f-4eb0-a4fc-3bb0b83fe3f4",
   "metadata": {},
   "source": [
    "Let's vary NEXT's search completeness (which is only practical in Salmon's system). That is, if the search evaluates $N$ queries per second, how does search performance vary? That is, let's vary `n_search` in this code:\n",
    "\n",
    "``` python\n",
    "def next_search(n_search=30):\n",
    "    queries = [_random_query(n=90) for _ in range(n_search)]\n",
    "    scores = [_info_gain(q) for q in queries]\n",
    "    \n",
    "    best_query = queries[np.argmax(scores)]\n",
    "    return best_query\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb48ac-7e7d-4269-99d2-25ebd5e8d51e",
   "metadata": {},
   "source": [
    "Let's vary the search by (roughly) a factor of 30, which simulates making NEXT's search 30× faster."
   ]
  },
  {
   "cell_type": "raw",
   "id": "91cf8c44-e74d-40c7-a1fb-bc0ec5efd4c8",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be9d675c-c4cc-4b7a-9c8a-7665e8da1114",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/search1.png}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1167ae24-5c8a-4181-9c1b-002e0019aae5",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed9e52-6c94-4768-8477-516e2208eaab",
   "metadata": {},
   "source": [
    "However, if NEXT's search is too complete, it tends to focus on a particular head:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32ca9b0d-a224-49e5-829d-7807e2947adf",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1951ef-0aec-4ada-8170-10ee0b936847",
   "metadata": {},
   "source": [
    "![](figs/n-run-linear.png)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42068299-eac4-4992-a3af-0c12f48a51d0",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e5510-86c2-4044-8615-d868c80eb995",
   "metadata": {},
   "source": [
    "For example, here's part of the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d8436c-fe9f-41ee-be0a-07e671f499a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 25, 78, 78, 78,\n",
       "       78, 78, 78])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "n_search = 30_000\n",
    "df = pd.read_csv(f\"salmon/io/2021-05-26-search/TSTE-n_search={n_search}-1_responses.csv.zip\")\n",
    "df[\"head\"].iloc[1000:1000 + 20].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597cfb0-8b3b-4cc3-b20f-7775221350f4",
   "metadata": {},
   "source": [
    "That means the 78th head object—alien egg 524—appeared in the head position for queries 1000 through 1020:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae4a2d-a159-4500-acbf-b4041cf758c0",
   "metadata": {},
   "source": [
    "# Salmon search validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e0a47-0581-4ccc-9f92-b4cfec7152a8",
   "metadata": {},
   "source": [
    "The issue above has motivated two changes to Salmon's search that allow for more item diversity in the queries. The search is modified to do the following:\n",
    "\n",
    "* Randomly selecting the \"head\" of the query\n",
    "* Randomly selecting one of the top-$k$ scoring queries for each head.\n",
    "\n",
    "In practice, one of $k n$ high scoring queries is randomly selected if there are $n$ target items. Now, these questions are relevant:\n",
    "\n",
    "1. Are the queries diverse enough?\n",
    "1. How much do the adaptive gains depend on search length?\n",
    "1. How much do the adaptive gains depend on the round-robin scheme with $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8feac5-8958-4b64-92fb-43e319cb3f2c",
   "metadata": {},
   "source": [
    "## Query diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60431f9-c7c5-494e-9ae4-51c5850252a7",
   "metadata": {},
   "source": [
    "The \"round robin\" queries are certainly a lot more diverse:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f86c24c-79fc-4e16-aec7-2aea118e1a61",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c435b81-9e65-4066-ab17-43ed18e95845",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/query_runs.png}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b524205b-43fd-46de-b31c-04211191c6ca",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ff66a-f91a-4d00-a0a2-df5d66eb58d7",
   "metadata": {},
   "source": [
    "This shows different search lengths and choosing the best query for each head. If the top $k$ queries are shown for the most complete search,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a2ba6-18e7-4e8c-85f9-34d057af6f8a",
   "metadata": {},
   "source": [
    "## Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8d1be-4cea-4271-91f4-67aee867c5cd",
   "metadata": {},
   "source": [
    "In practice, Salmon's search selects $k n$ high scoring queries with a randomly selected head.\n",
    "\n",
    "* How important is the random selection of the head?\n",
    "* How important is $k$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4233a-af01-4900-a425-fa21090d6fd2",
   "metadata": {},
   "source": [
    "### Round-robin search length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5585cf-88c4-4dd9-a105-2011269c472c",
   "metadata": {},
   "source": [
    "**How important is randomly rotating the head?** Unfortunately, when investigating this there's a confounding variable: the number of queries searched before randomly selecting a head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1225ac0-b86c-4228-ab31-52f743275b4c",
   "metadata": {},
   "source": [
    "First, let's see how much head rotation helps for a *fixed* number of queries searched:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc37c478-bd40-45c3-8a84-27a654aba508",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "126cee89-ec4d-48a0-a1a0-4e148e556d44",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/arr-tste-search.png}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a72da3a8-0308-4ee0-aff8-a269dc44aae1",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0916b166-fd2a-457d-9aad-503d2f4fc79d",
   "metadata": {},
   "source": [
    "It appears that randomly rotating the head is beneficial for any search length for both metrics, at least for these searches were the greedy search without head rotation performs decently.\n",
    "\n",
    "As such, Salmon randomly rotates the head because it helps for relatively limited searches. Now, let's look at different search lengths against the best possible TSTE performance:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ea7306d-d224-4a3b-a94f-dd57571cf1e9",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "767e31fb-3f37-47ea-929c-a8f00b454156",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/arr-search.png}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd1ede69-4777-438d-9128-8c62ca7efb4d",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c447e60-e1a3-46f9-81f7-8b0456d07718",
   "metadata": {},
   "source": [
    "For test accuracy, it looks like all searches except the most exhaustive search beat the best greedy search. This is likely due to some difficulties with embedding this one dimensional manifold into $d=2$ dimensions. In that case, looking at the avg. number of items closer than the true NN might be better because it's a measure of true embedding quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50b108-cd85-4f03-911e-f257fbf289d5",
   "metadata": {},
   "source": [
    "### Choosing one of top-$k$ queries for each head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64508e-9cb2-4256-895a-0d1c7f300b3c",
   "metadata": {},
   "source": [
    "In practice, the Salmon search is very efficient. For $n=90$ objects embedded into $d=2$ dimensions, it can score every query. In Salmon's search, one of the top-$k$ queries is selected for each head. **How does the value of $k$ affect the search?**\n",
    "\n",
    "Here's a graph that illustrates the performance of changing $k$ for the most complete search:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cddd1bd-c9fa-4643-b38c-f7a5bfe09d3e",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b765f7f6-70dd-4ce4-bdf3-44bcd047e44f",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/arr-n_top.png}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c84d95bb-2a47-4e01-929b-851326e27f95",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd86dd2-4d71-4dad-9abc-96267e5bf82e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Any value of $k \\le 30$ performs pretty much equivalently for accuracy, though any value of $k \\le 10$ perfoms equally well for nearest neighbor distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ab444-d605-49e9-b239-49caad04d724",
   "metadata": {},
   "source": [
    "This suggests a new architecture for NEXT that rotates the head randomly and searches about 400 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7946d2d-f944-42f3-bbc5-6b6af9d9bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391.6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 90\n",
    "k = 10\n",
    "possible_queries = (n - 1) * (n - 2) // 2\n",
    "possible_queries / k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842fe6c-e8cf-4199-b97d-faa74cafc212",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38baa0-be76-42df-a855-ad15f2992894",
   "metadata": {},
   "source": [
    "How does the complete Salmon search with `n_top=3` perform with the best of the 9 NEXT-like searches?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10bc0fd4-e3d5-4022-ab6c-bb5db4648a4f",
   "metadata": {},
   "source": [
    "\\begin{center}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3422b1d-30ba-4501-8cef-cf67828e0b61",
   "metadata": {},
   "source": [
    "\\includegraphics[width=0.7\\textwidth]{figs3/summary-complete.png}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c2a2f43-7b94-4c20-8834-ad33b19032a2",
   "metadata": {},
   "source": [
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d094335-437a-402c-bfe6-d697523ac3af",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7266c-a3c7-4959-bf4b-610be66bcc6e",
   "metadata": {},
   "source": [
    "We have presented evidence that suggests that NEXT's search presents experimental design challenges for exhaustive searches. To circumvent these issues, we have modified NEXT's search to include random head selection and random selection of the top-$k$ queries. Through some validation, it appears that the head rotation has the greatest affect on embedding quality, with smaller changes for search efficacy and the choice of $k$. The best adaptive performance is retained with an exhaustive search and $k=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b8c0c-bbf7-4f84-bdd3-73a6339d14ac",
   "metadata": {},
   "source": [
    "In addition, we have identified a new search architecture for NEXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96d37c-7363-452e-8a22-93215551cda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
